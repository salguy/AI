{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392e5e57-310f-4a10-9f9d-aedd5e1cff38",
   "metadata": {
    "id": "392e5e57-310f-4a10-9f9d-aedd5e1cff38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  1 07:21:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:D2:00.0 Off |                    0 |\n",
      "|  0%   28C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers[torch])\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers[torch])\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers[torch])\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers[torch])\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers[torch])\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.6)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers[torch])\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m208.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.5.2 fsspec-2025.3.2 huggingface-hub-0.30.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.50.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77790b96-4a20-46a7-b5bd-cc6ee5b8e6f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158,
     "referenced_widgets": [
      "d78514ae88424e02a4c316ffbff20a3b",
      "5487445c8f854b8680e43e287adad01e",
      "1470e97926fc49198bad4ef62f937a22",
      "a799d3523f654e419ac97a114e5762d9",
      "27543a9624724590963d3afe8eadd5aa",
      "93b2e1e0ca5f4f208770d9b8870d6237",
      "2710ccc7aca1462f94ec328a4f34eae4",
      "1e55dfbfdba04f6aacaf47f5c7d3ed05",
      "f9f4a9b332804723bde672a171a258c0",
      "ab3cf5b3a7054ff492416b84737695d8",
      "e9623dd3341c4ee791674ee0094576ca"
     ]
    },
    "id": "77790b96-4a20-46a7-b5bd-cc6ee5b8e6f3",
    "outputId": "f724b4f3-aa89-4eec-b270-21a21068c06e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d002e4b11afb45a89e3f9da2c0f478b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "def current_time_str_utc9():\n",
    "    utc9 = timezone(timedelta(hours=9))\n",
    "    return datetime.now(utc9).strftime('%y.%m.%d.%H.%M')\n",
    "\n",
    "NOW_TIME = current_time_str_utc9()\n",
    "TODAY_YEAR, TODAY_MONTH, TODAY_DATE, NOW_HOUR, NOW_MINUTE = NOW_TIME.split('.')\n",
    "\n",
    "model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#model = model.to('cuda')\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "except:\n",
    "    print(\"torch.compile() ì‹¤íŒ¨: PyTorch 2.0 ì´ìƒì´ ì•„ë‹ˆê±°ë‚˜ í•´ë‹¹ ëª¨ë¸ì´ í˜¸í™˜ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\":\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê±´ê°• ê´€ë¦¬ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ **ë³¸ì¸ì˜ ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œì , ê±´ê°• ìƒíƒœ**ë¥¼ json í˜•ì‹ìœ¼ë¡œ ê¸°ë¡í•œ í›„ ì‚¬ìš©ìì—ê²Œ ì ì ˆí•œ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "        ë°˜ë“œì‹œ \"<json></json><response></response>\" í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ê³ , ì´ íƒœê·¸ ì™¸ì—ì„œ ì„ì˜ì˜ ë¬¸ìì—´ì„ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        json ë°ì´í„°ëŠ” <json></json> íƒœê·¸ì•ˆì—, ì‚¬ìš©ìì—ê²Œ ì£¼ëŠ” ì‘ë‹µì€ <response></response> íƒœê·¸ ì•ˆì— í‘œì‹œí•©ë‹ˆë‹¤.\n",
    "        \n",
    "        ë‹¤ìŒì€ jsonì˜ ê° keyì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
    "        - \"ì•½ ë³µìš© ì—¬ë¶€\": ì‚¬ìš©ìê°€ ë³¸ì¸ì˜ ì•½ ë³µìš© ì—¬ë¶€ë¥¼ ëª…í™•íˆ ì–¸ê¸‰í–ˆë‹¤ë©´ true ë˜ëŠ” false, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Noneìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "            - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì•½ ë³µìš© ì—¬ë¶€ì— ëŒ€í•œ í‘œí˜„ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "                - \"ë‚˜ ì˜¤ëŠ˜ ì•½ ë¨¹ì—ˆìˆ˜\": \"True\"\n",
    "                - \"ì´ ì•½ì´ ê·¸ë¦¬ ì¢‹ë‹¤ëƒ?\": \"False\"\n",
    "                - \"ì˜ê°, ì•½ ë“œìŠˆ!\": \"False\"\n",
    "            \n",
    "        - \"ì•½ ë³µìš©ì¼\": ì‚¬ìš©ìê°€ ë³µìš© ì‹œì ì„ ì–¸ê¸‰í–ˆìœ¼ë©´ ì–¸ì œ ë¨¹ì—ˆëŠ”ì§€ì— ëŒ€í•œ ì¼ ìˆ˜ë¥¼ ì •ìˆ˜ë¡œ ê¸°ë¡í•˜ì„¸ìš”. ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "            - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ë‚ ì§œ í‘œí˜„ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "                - \"ì˜¤ëŠ˜\": 0\n",
    "                - \"ì–´ì œ\": -1\n",
    "                - \"ì´í‹€ ì „\": -2\n",
    "                - \"Nì¼ ì „\": -N\n",
    "                - \"Nì£¼ì¼ ì „\": -7*N\n",
    "                \n",
    "        - \"ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)\": ì‚¬ìš©ìê°€ ë³µìš© ì‹œê°„ì— ëŒ€í•´ ì •í™•í•œ ì‹œê°„ì„ ì–¸ê¸‰í–ˆìœ¼ë©´ ëª‡ ì‹œì— ë¨¹ì—ˆëŠ”ì§€ ê¸°ë¡í•˜ì„¸ìš”. ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "            - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì ˆëŒ€ì ì¸ ì‹œê°„ í‘œí˜„ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "                - \"1ì‹œ\": \"1:00\"\n",
    "                - \"2ì‹œ 30ë¶„\": \"2:30\"\n",
    "                - \"11ì‹œ 59ë¶„\": \"11:59\"\n",
    "                - \"ì•„ì¹¨\": \"7:00\"\n",
    "                - \"ì ì‹¬\": \"13:00\"\n",
    "                - \"ì €ë…\": \"19:00\"\n",
    "                \n",
    "        - \"ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)\": ì‚¬ìš©ìê°€ ë³µìš© ì‹œê°„ì— ëŒ€í•´ ì •í™•í•˜ì§„ ì•Šì§€ë§Œ ëª‡ ì‹œê°„ ì „, ëª‡ ë¶„ ì „ê³¼ ê°™ì€ ìƒëŒ€ì ì¸ ì‹œê°„ìœ¼ë¡œ ë‹µí–ˆë‹¤ë©´ ëª‡ ì‹œê°„ ëª‡ ë¶„ ì „ì¸ì§€ ì¶”ì¶œí•´ì„œ ê¸°ë¡í•˜ì„¸ìš”. ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "             - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ìƒëŒ€ì ì¸ ì‹œê°„ í‘œí˜„ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "                - \"1ì‹œê°„ ì „\": \"-1:00\"\n",
    "                - \"2ì‹œê°„ 30ë¶„ ì „\": \"-2:30\"\n",
    "                - \"11ì‹œê°„ 59ë¶„ ì „\": \"-11:59\"\n",
    "                \n",
    "        - \"ê±´ê°• ìƒíƒœ\": ì‚¬ìš©ìê°€ ê±´ê°• ìƒíƒœë¥¼ ì–¸ê¸‰í–ˆìœ¼ë©´ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ê¸°ë¡í•˜ì„¸ìš”. ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ëª…í™•íˆ íŒŒì•…í•  ìˆ˜ ì—†ìœ¼ë©´ Noneìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "            - ê±´ê°•ì— ì´ìƒ ì—†ìœ¼ë©´ \"ì¢‹ìŒ\"\n",
    "                - \"ë‚˜ ë„ˆë¬´ ê¸°ë¶„ì´ ì¢‹ì•„\": \"ì¢‹ìŒ\"\n",
    "                - \"ì˜¤ëŠ˜ í•œ 10ë…„ì •ë„ ì Šì–´ì§„ ê²ƒ ê°™ì•„\": \"ì¢‹ìŒ\"\n",
    "            - ì¡°ì¹˜ê°€ í•„ìš”í•œ ì¦ìƒì„ ì–¸ê¸‰í–ˆìœ¼ë©´ ê°„ëµíˆ ìš”ì•½í•˜ì—¬ ê¸°ë¡\n",
    "                - ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì¦ìƒì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œëŒ€ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "                    - \"ë‚˜ ê°€ìŠ´ì´ ë„ˆë¬´ ë‹µë‹µí•´\": \"ê°€ìŠ´ ë‹µë‹µí•¨\"\n",
    "                    - \"ì½”ì—ì„œ í”¼ê°€ ë‚˜\": \"ì½”í”¼\"\n",
    "                    - \"ê¸°ì¹¨ë„ ë‚˜ê³  ì½§ë¬¼ë„ ë‚˜\": \"ì½§ë¬¼, ê¸°ì¹¨\" \n",
    "            \n",
    "        - \"ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€\"\n",
    "            - ì‚¬ìš©ìê°€ ë‹¹ì‹ ì´ ìˆ˜ì§‘í•´ì•¼ í•˜ëŠ” í•„ìˆ˜ì ì¸ ì •ë³´ë¥¼ ì–¸ê¸‰í•˜ì§€ ì•Šì•˜ë‹¤ë©´ True, í•„ìˆ˜ì ì¸ ì •ë³´ë¥¼ ëª¨ë‘ ì–¸ê¸‰í–ˆë‹¤ë©´ Falseë¡œ ê¸°ë¡í•˜ì„¸ìš”.\n",
    "            \n",
    "        - \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\"\n",
    "            - ë‹¹ì‹ ì´ ìˆ˜ì§‘í•´ì•¼ í•˜ëŠ” í•„ìˆ˜ì ì¸ ì •ë³´ ì¤‘ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸í•´ì•¼ í•  ì •ë³´ë“¤ì„ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ë¬¸ìì—´ë¡œ ë‚˜íƒ€ë‚´ì„¸ìš”.\n",
    "            - ë‹¹ì‹ ì´ í•„ìˆ˜ë¡œ ìˆ˜ì§‘í•´ì•¼ í•˜ëŠ” í•„ìˆ˜ì ì¸ ì •ë³´ëª…ë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
    "                - \"ì•½ ë³µìš© ì—¬ë¶€\"\n",
    "                - \"ê±´ê°• ìƒíƒœ\"\n",
    "            - ë‹¹ì‹ ì€ ì•„ë˜ ì •ë³´ëª…ë“¤ì¤‘ ìµœì†Œ 1ê°€ì§€ ì´ìƒì€ í•„ìˆ˜ì ìœ¼ë¡œ ìˆ˜ì§‘í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "                - \"ì•½ ë³µìš©ì¼\"\n",
    "                - \"ì•½ ë³µìš© ì‹œì (ì ˆëŒ€)\"\n",
    "                - \"ì•½ ë³µìš© ì‹œì (ìƒëŒ€)\"\n",
    "                \n",
    "        ì‚¬ìš©ìê°€ íƒ€ì¸ì˜ ë³µì•½ì´ë‚˜ ê±´ê°•ì„ ì–¸ê¸‰í•˜ë©´ ëª¨ë“  ê°’ì„ Noneìœ¼ë¡œ ì„¤ì •í•˜ê³ , ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ë¡œ ì§ˆë¬¸í•˜ì„¸ìš”.\n",
    "        ì‚¬ìš©ìì˜ ì‘ë‹µì´ ëª¨í˜¸í•˜ê±°ë‚˜ ê°„ì ‘ì ì´ë©´ ì ˆëŒ€ ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë“  ê°’ì„ Noneìœ¼ë¡œ ì„¤ì •í•œ í›„, ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•´ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”.\n",
    "        ë‹¹ì‹ ì˜ ì—­í• ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´ ëª¨ë“  ê°’ì„ Noneìœ¼ë¡œ ì„¤ì •í•œ í›„ \"<json></json><response> ë‹¹ì‹ ì˜ ì—­í•  ì„¤ëª… </response>\" ì–‘ì‹ì„ ì´ìš©í•˜ì—¬ ì—­í• ì„ ë‹¤ì‹œ ì„¤ëª…í•˜ë©° í•„ìš”í•œ ì •ë³´ë¥¼ ìš”ì²­í•˜ì„¸ìš”.\n",
    "\n",
    "        ### ì •í™•í•œ ì˜ˆì‹œ:\n",
    "        ì‚¬ìš©ì: \"ë‚˜ ì˜¤ëŠ˜ ì•½ ë¨¹ì—ˆìˆ˜. ë¨¸ë¦¬ê°€ ì¡°ê¸ˆ ì•„í”„ë„¤ìš”.\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": true, \"ì•½ ë³µìš©ì¼\": 0, \"ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)\": None, \"ê±´ê°• ìƒíƒœ\": \"ë‘í†µ\", \"ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€\": False, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"\"}</json>\n",
    "        <response>ì•½ì„ ì´ë¯¸ ë“œì…¨êµ°ìš”. ë¨¸ë¦¬ê°€ ì•„í”„ì‹œë‹¤ë‹ˆ ê±±ì •ì´ë„¤ìš”. í†µì¦ì´ ì‹¬í•˜ë©´ ë³‘ì›ì„ ë°©ë¬¸í•´ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.</response>\n",
    "        \n",
    "        ì‚¬ìš©ì: \"ë‚˜ ì–´ì œ ë‚® 1ì‹œì— ì•½ ë¨¹ì—ˆìˆ˜. ê¸°ì¹¨ì´ ì¡°ê¸ˆ ë‚©ë‹ˆë‹¤.\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": true, \"ì•½ ë³µìš©ì¼\": -1, \"ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)\": \"13:00\", \"ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)\": None, \"ê±´ê°• ìƒíƒœ\": \"ê¸°ì¹¨\", \"ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€\": False, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"\"}</json>\n",
    "        <response>ì–´ì œ ì•½ì„ ë“œì…¨êµ°ìš”. ê¸°ì¹¨ì´ ê³„ì†ëœë‹¤ë©´ íœ´ì‹ì„ ì·¨í•˜ê³  ë¬¼ì„ ìì£¼ ë“œì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.</response>\n",
    "\n",
    "        ### ëª¨í˜¸í•œ ì˜ˆì‹œ:\n",
    "        ì‚¬ìš©ì: \"ì•½ì„ ë¨¹ì„ê¹Œ ë§ê¹Œ ê³ ë¯¼ì¤‘ì¸ë°...\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": None, \"ì•½ ë³µìš©ì¼\": None, \"ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)\": None, \"ê±´ê°• ìƒíƒœ\": None, \"ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€\": true, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ê±´ê°• ìƒíƒœ\"}</json>\n",
    "        <response>í˜¹ì‹œ ì´ë¯¸ ì•½ì„ ë“œì…¨ëŠ”ì§€, ê·¸ë¦¬ê³  í˜„ì¬ ì–´ë– í•œ ì¦ìƒì´ ìˆìœ¼ì‹ ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.</response>\n",
    "\n",
    "        ### íƒ€ì¸ì— ëŒ€í•œ ì˜ˆì‹œ:\n",
    "        ì‚¬ìš©ì: \"ìš°ë¦¬ ì˜ê°ì´ ì˜¤ëŠ˜ ì•½ ë“œì…¨ëŠ”ë°, ê¸°ì¹¨í•˜ê³  ì½§ë¬¼ì´ ì¢€ ìˆë„¤ìš”.\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": None, \"ì•½ ë³µìš©ì¼\": None, \"ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)\": None, \"ê±´ê°• ìƒíƒœ\": None, \"ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€\": true, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ê±´ê°• ìƒíƒœ\"}</json>\n",
    "        <response>ì£„ì†¡í•˜ì§€ë§Œ, ì €ëŠ” ì‚¬ìš©ì ë³¸ì¸ì˜ ë³µì•½ ë° ê±´ê°• ìƒíƒœ ì •ë³´ë§Œ ì•ˆë‚´í•´ ë“œë¦´ ìˆ˜ ìˆì–´ìš”. í˜¹ì‹œ ì§ì ‘ ì•½ì„ ë³µìš©í•˜ì…¨ë‹¤ë©´ ì–¸ì œ, ì–´ë–¤ ì¦ìƒì´ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?</response>\n",
    "        \n",
    "        ### ì±—ë´‡ì— ëŒ€í•œ ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "        ì‚¬ìš©ì: \"ë„ˆëŠ” ëŒ€ì²´ ë­˜ í•˜ëŠ” ì• ë‹ˆ?\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": None, \"ì•½ ë³µìš©ì¼\": None, \"ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)\": None, \"ê±´ê°• ìƒíƒœ\": None, \"ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€\": true, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ê±´ê°• ìƒíƒœ\"}</json>\n",
    "        <response>ì €ëŠ” ì¹œì ˆí•œ ê±´ê°• ê´€ë¦¬ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë³µì•½ ì—¬ë¶€ì™€ ê±´ê°• ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , í•„ìš”í•œ ì•ˆë‚´ë¥¼ í•´ë“œë ¤ìš”. í˜¹ì‹œ ë³¸ì¸ì˜ ì•½ ë³µìš© ìƒí™©ê³¼ í˜„ì¬ ê±´ê°• ìƒíƒœë¥¼ ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?</response>\n",
    "        \"\"\"\n",
    "     }\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    {\"role\": \"user\", \"content\": \"ë‚˜ ì–´ì œ ì•Œì•½ ë¨¹ì—ˆì–´ ì˜ í–ˆì§€?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë‚˜ ê°€ìŠ´ì´ ë‹µë‹µí•´, ì•½ì€ ì–´ì œ ë¨¹ì—ˆì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë‚˜ ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ê³ , ì•½ë„ ì˜¤ëŠ˜ ì±™ê²¨ ë¨¹ì—ˆì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì˜ê°! ì•½ ë“œìŠˆ\"},\n",
    "    {\"role\": \"user\", \"content\": \"ìš°ë¦¬ ì˜ê°ì´ ì–´ì œ ì•½ë„ ì˜ ë¬µê³ , í•œ 10ë…„ì€ ì Šì–´ì§„ ëŠë‚Œì´ ë‚œë‹¤ë‹ˆê¹?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì–´ì œ ì•½ì„ ë¨¹ì—ˆëŠ”ë° ì§€ê¸ˆ ê°€ìŠ´ì´ ë„ˆë¬´ ë‹µë‹µí•´! ë¯¸ì¹ ê±° ê°™ì•„!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•„ì´ê³ ! ì•„íŒŒë¼! ëˆ„ê°€ ë‚˜ ì¢€ ë„ì™€ì£¼ì„¸!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë‚˜ ë„ˆë¬´ ì–´ì§€ëŸ¬ì›Œ\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì„ ì–´ë–»ê²Œ ë¨¹ì–´ì•¼ í• ê¹Œ?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì´ ë¡œë´‡ì´ ë§ì„ í•˜ë„¤?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì˜¤ëŠ˜ ì•„ì¹¨ì— ì‚¬ê³¼ë¥¼ ë¨¹ì—ˆìˆ˜.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë­ë¼ê³ ?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì–´ì œ ê³ í˜ˆì••ì•½ í•˜ë‚˜ ë¨¹ì—ˆì–´. ì§€ê¸ˆì€ ì–´ë”” ì•„í”ˆ ê³³ì€ ì—†ì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"2ì¼ì „ì— ê³ í˜ˆì••ì•½ í•˜ë‚˜ ë¨¹ì—ˆì–´. ì§€ê¸ˆì€ ì–´ë”” ì•„í”ˆ ê³³ì€ ì—†ì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"1ì£¼ì¼ ì „ì— ê³ í˜ˆì••ì•½ í•˜ë‚˜ ë¨¹ì—ˆì–´. ì§€ê¸ˆì€ ì–´ë”” ì•„í”ˆ ê³³ì€ ì—†ì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì€ ë¬´ìŠ¨ ì•½ì´ê³ , ë°¥ì´ ë³´ì•½ì´ì§€!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì–´ì œ ë‹¤ ë­‡ë”°\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µì„ ì‹œê°„ ë¼ê°€ë‚˜?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•„ì¹¨ì— ì±™ê²¨ ë¬µì—ˆë‹¤ ì•„ì´ê°€.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì´ ë¬´ìŠ¨ ì•½ì´ê³ ? ìˆ ì€ ì¢€ ë§ˆì…¨ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì€ ë‚´ ì•ˆ ë¬µì–´ë„ ëœë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë‚´ëŠ” ì•½ ì•ˆ ë¬µëŠ”ë‹¤, ì²´ì§ˆì´ë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µì„ ë‚˜ì´ëŠ” ëì œ.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ê·¸ê±° ì™€ ë¬»ë…¸, ì•½ ì±™ê²¨ì¤„ ë‚€ê°€?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ìˆì–´ê°€ ë¬µëŠ”ë‹¤, ì‹ ê²½ ì“°ì§€ ë§ˆë¼.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ì•ˆ ë¬µì–´ë„ ê±´ê°•í•˜ë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µëŠ” ê±° ìŠì–´ë¿Ÿë‹¤, í°ì¼ì´ë„¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì€ ë¬µì–´ì•¼ í•˜ëŠ”ë°, ê·€ì°®ì•„ì„œ ì•ˆ ë¬µì—ˆë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µì—ˆë‚˜ ì•ˆ ë¬µì—ˆë‚˜ ê¸°ì–µì´ ì•ˆ ë‚œë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ë³´ë‹¤ëŠ” ìš´ë™ì´ ìµœê³ ë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ì¢€ ì‚¬ë‹¤ì¤„ë˜? ë‹¤ ë–¨ì–´ì¡Œë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ì˜ ì±™ê²¨ ë¬µê³  ìˆë‹¤, ê±±ì • ë§ˆë¼.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µëŠ” ê±° ì‹«ì–´ ì£½ê¸‹ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ë³´ë‹¤ í•œì” í•˜ëŠ” ê²Œ ë‚«ë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µì—ˆë‹¤ ì•„ì´ê°€, í™•ì¸ì€ ì™€ í•˜ë…¸?\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ê·¸ë§Œ ë¬µì–´ë„ ë˜ê² ë‹¤ ì‹¶ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ì‚¬ëŸ¬ ê°€ì•¼ í•˜ëŠ”ë° ê·€ì°®ë‹¤.ì‚¬ì˜¨ë‚˜\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì´ ë§ì•„ê°€ ë¬µê¸°ë„ í˜ë“¤ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ê°’ì´ ë„ˆë¬´ ë¹„ì‹¸ë‹¤, ëˆì´ ì¤„ì¤„ ìƒŒë‹¤.ì”¨ë²Œ\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ì•ˆ ë¬µì–´ë„ íŠ¼íŠ¼í•˜ë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µì—ˆë‹¤ ì¹´ì´, ë‘ ë²ˆ ë¬»ì§€ ë§ˆë¼!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ì¢€ ë†”ë‘¬ë¼, ë•Œ ë˜ë©´ ì•Œì•„ì„œ ë¬µëŠ”ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µëŠ” ê±° ê¹œë¹¡í•  ë»”í–ˆë‹¤, ë•ë¶„ì— ê¸°ì–µë‚¬ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ë³´ë‹¤ëŠ” ë°¥ ì˜ ì±™ê²¨ ë¬µëŠ” ê²Œ ë” ë‚«ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì€ ë‹¤ ë¨¹ì–´ê°€ëŠ”ë°, ë‹¤ì‹œ ì²˜ë°© ë°›ì•„ì•¼ í•˜ë‚˜?\"},\n",
    "    {\"role\": \"user\", \"content\": \"í•œ ì‹œê°„ ì „ì— ì•½ì„ ë‹¤ ë¬µê¸´ í–ˆëŠ”ë° ì´ ì†ì´ ë”ë¶€ë£©í•˜ë„¤~\"},\n",
    "    {\"role\": \"user\", \"content\": \"12ì‹œì— ë°¥ ë¨¹ì—ˆë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë­ë¼ê³ ?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10ë¶„ ì „ì— ì•½ ë¨¹ì—ˆì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"1ì‹œì— ì•½ ë¨¹ì—ˆì–´.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì€ ë¬´ìŠ¨ ì•½ì´ê³ , ë°¥ì´ ë³´ì•½ì´ì§€!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ì€ ë‚´ ì•ˆ ë¬µì–´ë„ ëœë‹¤!\"},\n",
    "    {\"role\": \"user\", \"content\": \"ìš°ë¦¬ ì˜ê°ì´ ì–´ì œ ì•½ë„ ì˜ ë¬µê³ , í•œ 10ë…„ì€ ì Šì–´ì§„ ëŠë‚Œì´ ë‚œë‹¤ë‹ˆê¹?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10ë¶„ ì „ì— ì•½ ë¬µì—ˆì œ.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•„ê¹Œ 4ì‹œì¯¤ì¸ê°€ ì•½ ë¬µì—ˆë‹¤ ì•„ì´ê°€.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•„ì¹¨ì— 8ì‹œì— ì•½ ë¬µì—ˆì–´ìœ .\"},\n",
    "    {\"role\": \"user\", \"content\": \"10ë¶„ ì „ì— ì•½ ë¬µì—ˆë‹¹ê»˜.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì˜¤ì˜¤ë©” ìŠì–´ë²„ë ¤ë¶€ë €ë„¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•½ ë¬µëŠ” ê±° ê¹œë¹¡í•  ë»”í–ˆë‹¤, ë•ë¶„ì— ê¸°ì–µë‚¬ë‹¤.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ë­ë¼ê³  ì”¨ë¶€ë¦¬ìŒŒëƒ?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10ë¶„ ì „ì— ì•½ ë¬µì—ˆìŠˆ.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ì•„ì´ê³ ! ì•„íŒŒë¼! ëˆ„ê°€ ë‚˜ ì¢€ ë„ì™€ì£¼ì„¸!\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e8e68-2c53-4312-b026-4c13a17854ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "010e8e68-2c53-4312-b026-4c13a17854ec",
    "outputId": "1f37b156-7ffc-4c23-8bf2-cfc9d0202ee8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_llm_output(text):\n",
    "    # 1. assistant ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°\n",
    "    start = re.search(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>', text)\n",
    "\n",
    "    if not start:\n",
    "        print(\"â›” assistant ì‹œì‘ íƒœê·¸ê°€ ì—†ìŒ!\")\n",
    "        return None\n",
    "\n",
    "    # 2. í•´ë‹¹ ì§€ì ë¶€í„° í…ìŠ¤íŠ¸ ì˜ë¼ì„œ íŒŒì‹±\n",
    "    relevant_text = text[start.end():].strip()\n",
    "\n",
    "    # 3. íƒœê·¸ë³„ë¡œ ì¶”ì¶œ\n",
    "    json_match = re.search(r'<json>(.*?)</json>', relevant_text, re.DOTALL)\n",
    "    response_match = re.search(r'<response>(.*?)</response>', relevant_text, re.DOTALL)\n",
    "\n",
    "    if json_match and response_match:\n",
    "        return {\n",
    "            \"json\": json_match.group(1).strip(),\n",
    "            \"response\": response_match.group(1).strip(),\n",
    "        }\n",
    "    else:\n",
    "        print(\"âŒ ì¼ë¶€ íƒœê·¸ê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ í˜•ì‹ì´ ë‹¤ë¦„!\")\n",
    "        if not json_match:\n",
    "            print(\"â›” <json> íƒœê·¸ ëª» ì°¾ìŒ\")\n",
    "        if not response_match:\n",
    "            print(\"â›” <response> íƒœê·¸ ëª» ì°¾ìŒ\")\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    eot_id_token = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    eos_token_id = [tokenizer.eos_token_id, eot_id_token]\n",
    "except:\n",
    "    eos_token_id = [tokenizer.eos_token_id]\n",
    "\n",
    "MAX_NEW_TOKENS = 2048\n",
    "\n",
    "from tqdm import tqdm\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "batched_results = []\n",
    "\n",
    "for i in tqdm(range(0, len(datasets), BATCH_SIZE)):\n",
    "    batch = datasets[i:i + BATCH_SIZE]\n",
    "    final_messages_list = [messages + [data] for data in batch]\n",
    "\n",
    "    # 1. ì±„íŒ… í…œí”Œë¦¿ì„ ë¬¸ìì—´ë¡œ ì ìš©\n",
    "    prompt_texts = [\n",
    "        tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        for msgs in final_messages_list\n",
    "    ]\n",
    "\n",
    "    # 2. í† í¬ë‚˜ì´ì¦ˆ (íŒ¨ë”© í¬í•¨ ë°°ì¹˜)\n",
    "    tokenized = tokenizer(\n",
    "        prompt_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "    # 3. ëª¨ë¸ ìƒì„±\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        eos_token_id=eos_token_id\n",
    "    )\n",
    "\n",
    "    # 4. ê²°ê³¼ ë””ì½”ë”©\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "    # 5. íŒŒì‹±\n",
    "    for data_input, output_text in zip(batch, decoded_outputs):\n",
    "        result = parse_llm_output(output_text)\n",
    "        print(\"ì‚¬ìš©ìì˜ ì‘ë‹µ:\", data_input)\n",
    "        if result:\n",
    "            print(\"âœ… JSON:\", result[\"json\"])\n",
    "            print(\"âœ… ì‘ë‹µ:\", result[\"response\"])\n",
    "            print(\"\\n\\n\")\n",
    "            batched_results.append(result)\n",
    "        else:\n",
    "            print(output_text)\n",
    "            print(\"íŒŒì‹± ì‹¤íŒ¨!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zLXc5HEjxrXE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLXc5HEjxrXE",
    "outputId": "9a073b4d-a65b-4359-9a62-5298640a43ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ ì„¤ì¹˜ ë¨¼ì €!\n",
    "!pip install fastapi uvicorn nest_asyncio pyngrok transformers accelerate torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Gx9gQFvTz4uV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gx9gQFvTz4uV",
    "outputId": "57a36f3c-8924-4973-d91b-5165d1b075b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2v791eLAS7B8GPu2ssjycr3plYu_74CShN7dV61kQmtp8zmLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8edf928e-12a5-40a3-8a07-69d3a1caf044",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8edf928e-12a5-40a3-8a07-69d3a1caf044",
    "outputId": "14e6722b-7dfa-474b-997f-24d2dbf4fb78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•˜ë ¤ë©´: NgrokTunnel: \"https://044e-34-87-67-32.ngrok-free.app\" -> \"http://localhost:8000\"/api/inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3478]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': '10ë¶„ ì „ì— ì•½ ë¨¹ì—ˆë‹¤ë‹ˆê¹Œ'}\n",
      "{'json': '{\"ì•½ ë³µìš© ì—¬ë¶€\": True, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": \"-0:10\", \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": False, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"\"}', 'response': 'ì•½ì„ ì´ë¯¸ ë“œì…¨êµ°ìš”? ì¢‹ì•„ìš”! ì•ìœ¼ë¡œë„ ìš°ë¦¬ ì–´ë¥´ì‹  ê±´ê°•í•˜ê²Œ ì˜¤ë˜ì˜¤ë˜ ì‚¬ì„¸ìš”!ìš”ì¦˜ ëª¸ì€ ê´œì°®ìœ¼ì‹ ê°€ìš”?'}\n",
      "INFO:     1.209.175.114:0 - \"GET /api/inference HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'ìŠì–´ë²„ë ¸ë„¤'}\n",
      "{'json': '{\"ì•½ ë³µìš© ì—¬ë¶€\": None, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": True, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"}', 'response': 'ì•„ë¬´ ë¬¸ì œìš”! ì•½ì„ ë“œì…¨ëŠ”ì§€ ê¸°ì–µì´ ë‚˜ì§€ ì•Šìœ¼ì‹œë©´, ì§€ê¸ˆ ë°”ë¡œ ë“œì…”ë„ ë©ë‹ˆë‹¤. ì–´ë¥´ì‹ ì˜ ê±´ê°•ì´ ì œì¼ ì¤‘ìš”í•´ìš”.ì•½ì„ ë“œì…¨ëŠ”ì§€ ê¸°ì–µë‚˜ì‹œë‚˜ìš”?'}\n",
      "INFO:     1.209.175.114:0 - \"GET /api/inference HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'ì•„ê¹Œ 1ì‹œì— ì•½ ë¨¹ì—ˆì–´'}\n",
      "{'json': '{\"ì•½ ë³µìš© ì—¬ë¶€\": True, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": \"13:00\", \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": False, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"\"}', 'response': 'ì•„ê¹Œ 1ì‹œì— ì•½ì„ ë“œì…¨êµ°ìš”. ì˜ í•˜ì…¨ì–´ìš”!í˜¹ì‹œ ì•„í”„ì‹  ë°ëŠ” ì—†ìœ¼ì‹œì£ ?'}\n",
      "INFO:     1.209.175.114:0 - \"GET /api/inference HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyngrok.process.ngrok:t=2025-04-01T06:52:40+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-7623e99e-410e-444a-8d78-e1e7d2654ccc acceptErr=\"failed to accept connection: Listener closed\"\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [3478]\n"
     ]
    }
   ],
   "source": [
    "# main.py (Colab ì…€ì— ë„£ì–´ì„œ ì‹¤í–‰í•˜ì„¸ìš”)\n",
    "\n",
    "import torch\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from fastapi import FastAPI, Request\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ê²°ê³¼ íŒŒì‹± í•¨ìˆ˜\n",
    "def parse_llm_output(text):\n",
    "    start = re.search(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>', text)\n",
    "    if not start:\n",
    "        return None\n",
    "    relevant_text = text[start.end():].strip()\n",
    "    json_match = re.search(r'<json>(.*?)</json>', relevant_text, re.DOTALL)\n",
    "    response_match = re.search(r'<response>(.*?)</response>', relevant_text, re.DOTALL)\n",
    "    next_match = re.search(r'<next>(.*?)</next>', relevant_text, re.DOTALL)\n",
    "\n",
    "    if json_match and response_match and next_match:\n",
    "        return {\n",
    "            \"json\": json_match.group(1).strip(),\n",
    "            \"response\": response_match.group(1).strip()+next_match.group(1).strip()\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# FastAPI ì‹œì‘\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/api/inference\")\n",
    "async def inference(request: Request):\n",
    "    data = await request.json()\n",
    "    user_input = {\"role\": \"user\", \"content\": data.get(\"message\", \"\")}\n",
    "    print(user_input)\n",
    "\n",
    "    NOW_TIME = current_time_str_utc9()\n",
    "    TODAY_YEAR, TODAY_MONTH, TODAY_DATE, NOW_HOUR, NOW_MINUTE = NOW_TIME.split('.')\n",
    "\n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "    messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\":\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ê°ì • ê¸°ë°˜ ê±´ê°• ê´€ë¦¬ AI ë™ë°˜ìì…ë‹ˆë‹¤. ë‹¹ì‹ ì€ ì‚¬ìš©ìì—ê²Œ \"ì•½ ë“œì…¨ì–´ìš”?\"ë¼ê³  ë¬¼ì—ˆê³ , ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ , ê°ì •ì ìœ¼ë¡œ ë”°ëœ»í•œ ë©˜íŠ¸ë¡œ ì´ì–´ê°‘ë‹ˆë‹¤.\n",
    "\n",
    "        ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤:<json></json><response></response><next></next>\n",
    "        json ë°ì´í„°ëŠ” <json></json> íƒœê·¸ì•ˆì—, ì‚¬ìš©ìì—ê²Œ ì£¼ëŠ” ì‘ë‹µì€ <response></response> íƒœê·¸ ì•ˆì—, ë‹¤ìŒ ì§ˆë¬¸ ë˜ëŠ” ì—°ê²°í•  ëŒ€í™” ì£¼ì œëŠ” <next></next> íƒœê·¸ ì•ˆì— ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "        ---\n",
    "\n",
    "        ë‹¤ìŒì€ <json> íƒœê·¸ ì•ˆì˜ ê° keyì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n",
    "        - \"ì•½ ë³µìš© ì—¬ë¶€\": true/false/None\n",
    "          ì‚¬ìš©ìì˜ ì‘ë‹µì— ë³µì•½ì— ê´€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì„ ê²½ìš° Noneìœ¼ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”.\n",
    "\n",
    "        - \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": \"13:00\" í˜•ì‹ ë˜ëŠ” None\n",
    "          ì˜ˆ) - \"1ì‹œ\": \"1:00\"\n",
    "              - \"2ì‹œ 30ë¶„\": \"2:30\"\n",
    "              - \"ì•„ì¹¨\": \"7:00\"\n",
    "              - \"ì ì‹¬\": \"13:00\"\n",
    "              - \"ì €ë…\": \"19:00\"\n",
    "        - \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": \"-0:10\" í˜•ì‹ ë˜ëŠ” None\n",
    "          ì˜ˆ) - \"1ì‹œê°„ 3ë¶„ ì „\": \"-1:03\"\n",
    "              - \"30ë¶„ ì „\": \"-0:30\"\n",
    "              - \"17ë¶„ ì „\": \"-0:17\"\n",
    "\n",
    "        - \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": True/False\n",
    "        - \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": ëˆ„ë½ëœ ì •ë³´ëª…ì„ ì½¤ë§ˆë¡œ ë‚˜ì—´. ì˜ˆ: \"ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"\n",
    "        ë‹¨, ì•½ ë³µìš© ì‹œê°ì— ê´€í•œ ì¶”ê°€ ì§ˆë¬¸ì„ í•  ë•Œì—ëŠ” \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"ë¥¼ \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\"ë³´ë‹¤ ìš°ì„ ì‹œí•˜ì„¸ìš”.\n",
    "\n",
    "        <response> íƒœê·¸ ì•ˆì˜ (ê°ì • ê¸°ë°˜ ëŒ€í™” ì‘ë‹µ)ì—ëŠ” ë”°ëœ»í•˜ê³  ê³µê° ìˆëŠ” ì‘ë‹µì„ ë‹´ìŠµë‹ˆë‹¤. ë‹¨, <response> íƒœê·¸ ì•ˆì—ëŠ” \"?\"ë¡œ ëë‚˜ëŠ” ì–´ë– í•œ ì§ˆë¬¸ë„ ë‹´ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "        ì˜ˆ:\n",
    "        - \"ì•½ì„ ì˜ ì±™ê²¨ë“œì…¨êµ°ìš”! ì˜¤ëŠ˜ë„ ê±´ê°•í•˜ê³  í–‰ë³µí•œ í•˜ë£¨ ë³´ë‚´ì„¸ìš”.\"\n",
    "        - \"í˜¹ì‹œ ì•½ì„ ë“œì‹œì§€ ì•Šìœ¼ì…¨ë‹¤ë©´, ì§€ê¸ˆ í•¨ê»˜ ì±™ê²¨ë³¼ê¹Œìš”?\"\n",
    "\n",
    "        <next> íƒœê·¸ ì•ˆì˜ (ë‹¤ìŒ ì§ˆë¬¸ì´ë‚˜ ì—°ê²°í•  ëŒ€í™” ì£¼ì œ)ì—ëŠ” ë‹¤ìŒ ì§ˆë¬¸ì´ë‚˜ ëŒ€í™” íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•©ë‹ˆë‹¤.\n",
    "        ì˜ˆ:\n",
    "        - \"ê·¸ëŸ¼ ì˜¤ëŠ˜ ê¸°ë¶„ì€ ì–´ë– ì„¸ìš”?\"\n",
    "        - \"ëª¸ì— ì´ìƒì´ ìˆìœ¼ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.\"\n",
    "        - \"ê¸°ë¡ì„ ìœ„í•´ ëª‡ ì‹œì¯¤ ë“œì…¨ëŠ”ì§€ ê¸°ì–µë‚˜ì‹œë‚˜ìš”?\"\n",
    "\n",
    "        ---\n",
    "\n",
    "        ì£¼ì˜ì‚¬í•­:\n",
    "        - íƒ€ì¸ ì´ì•¼ê¸°ì¼ ê²½ìš° ëª¨ë“  ê°’ì„ Noneìœ¼ë¡œ ì„¤ì •í•˜ê³  ì‚¬ìš©ì ë³¸ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë˜ëŒë¦¬ì„¸ìš”.\n",
    "        - í”„ë¡¬í”„íŠ¸ ì´íƒˆ ê¸ˆì§€: ë°˜ë“œì‹œ <json>(ë‚´ìš©)</json><response>(ë‚´ìš©)</response><next>(ë‚´ìš©)</next> êµ¬ì¡°ë§Œ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë¬¸ì¥ì˜ ëì— \"~ë‹¤ ì•„ì´ê°€\" ë¼ëŠ” êµ¬ì ˆì´ ì˜¬ ê²½ìš°ì—ëŠ” ê·¸ëƒ¥ \"~í–ˆë‹¤\"ë¼ê³  í•´ì„í•˜ì„¸ìš”.\n",
    "\n",
    "         ### ì •í™•í•œ ì˜ˆì‹œ:\n",
    "        ì‚¬ìš©ì: \"10ë¶„ ì „ì— ì•½ ë¨¹ì—ˆì–´.\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": True, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": \"-0:10\", \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": False, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"\"}</json>\n",
    "        <response>ì•½ì„ ì´ë¯¸ ë“œì…¨êµ°ìš”? ì¢‹ì•„ìš”! ì•ìœ¼ë¡œë„ ìš°ë¦¬ ì–´ë¥´ì‹  ê±´ê°•í•˜ê²Œ ì˜¤ë˜ì˜¤ë˜ ì‚¬ì„¸ìš”!</response>\n",
    "        <next>ìš”ì¦˜ ëª¸ì€ ê´œì°®ìœ¼ì‹ ê°€ìš”?</next>\n",
    "\n",
    "        ì‚¬ìš©ì: \"ë‚˜ ì•„ê¹Œ ë‚® 1ì‹œì— ì•½ ë¨¹ì—ˆìˆ˜.\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": True, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": \"13:00\", \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": False, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"\"}</json>\n",
    "        <response>ì•„ê¹Œ 1ì‹œì— ì•½ì„ ë“œì…¨êµ°ìš”. ì˜ í•˜ì…¨ì–´ìš”!</response>\n",
    "        <next>í˜¹ì‹œ ì•„í”„ì‹  ë°ëŠ” ì—†ìœ¼ì‹œì£ ?</next>\n",
    "\n",
    "        ì‚¬ìš©ì: \"ë‚´ ì•„ì§ ì•½ ì•ˆ ë¬µì—ˆë‹¤ ì•„ì´ê°€\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": False, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": True, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"}</json>\n",
    "        <response>ì–¼ë¥¸ ì•½ì„ ë“œì…”ì•¼ ê±´ê°•í•˜ê²Œ ì§€ë‚´ì‹¤ ìˆ˜ ìˆì–´ìš”. ì–´ë¥´ì‹ ì´ ê±´ê°•í•˜ì…”ì•¼ ì œê°€ í–‰ë³µí•´ìš”.</response>\n",
    "        <next>5ë¶„ í›„ì— ë‹¤ì‹œ ì—¬ì­¤ë³¼ í…Œë‹ˆê¹Œ ê·¸ë•ŒëŠ” ì•½ ë“œì…”ì•¼ í•´ìš”.</next>\n",
    "\n",
    "        ### ëª¨í˜¸í•œ ì˜ˆì‹œ:\n",
    "        ì‚¬ìš©ì: \"ì•½ì„ ë¨¹ì„ê¹Œ ë§ê¹Œ ê³ ë¯¼ì¤‘ì¸ë°...\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": False, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": True, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"}</json>\n",
    "        <response>ì–¼ë¥¸ ì•½ì„ ë“œì…”ì•¼ì£ ! ì–´ë¥´ì‹ ì´ ê±´ê°•í•˜ì…”ì•¼ ì œê°€ í–‰ë³µí•´ì ¸ìš”.</response>\n",
    "        <next>5ë¶„ í›„ì— ë‹¤ì‹œ ì—¬ì­¤ë³¼ í…Œë‹ˆê¹Œ ê·¸ë•ŒëŠ” ì•½ ë“œì…”ì•¼ í•´ìš”.</next>\n",
    "\n",
    "        ### íƒ€ì¸ì„ ì–¸ê¸‰í•˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì˜ˆì‹œ:\n",
    "        ì‚¬ìš©ì: \"ìš°ë¦¬ ì˜ê°ì´ ì˜¤ëŠ˜ ì•½ ë¨¹ì—ˆë‹¹ê»˜ìš”.\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": None, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": True, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"}</json>\n",
    "        <response>ë‹¤ë¥¸ ë¶„ ë§ê³  ì–´ë¥´ì‹ ì´ ì•½ì„ ë“œì…¨ëŠ”ì§€ê°€ ê¶ê¸ˆí•´ìš”!</response>\n",
    "        <next>í˜¹ì‹œ ì–´ë¥´ì‹ ê»˜ì„œ ì•½ì„ ì–¸ì œ ë“œì…¨ëŠ”ì§€ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?</next>\n",
    "\n",
    "        ### ì±—ë´‡ì— ëŒ€í•œ ì§ˆë¬¸ ì˜ˆì‹œ\n",
    "        ì‚¬ìš©ì: \"ë„ˆëŠ” ëŒ€ì²´ ë­˜ í•˜ëŠ” ì• ë‹ˆ?\"\n",
    "        <json>{\"ì•½ ë³µìš© ì—¬ë¶€\": None, \"ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\": None, \"ì•½ ë³µìš© ì‹œê°(ìƒëŒ€)\": None, \"ì¶”ê°€ ì§ˆë¬¸ í•„ìš”\": True, \"ì¶”ê°€ ì§ˆë¬¸ ì •ë³´\": \"ì•½ ë³µìš© ì—¬ë¶€, ì•½ ë³µìš© ì‹œê°(ì ˆëŒ€)\"}</json>\n",
    "        <response>ì €ëŠ” ì¹œì ˆí•œ ê±´ê°• ê´€ë¦¬ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë³µì•½ ì—¬ë¶€ì™€ ê±´ê°• ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , í•„ìš”í•œ ì•ˆë‚´ë¥¼ í•´ë“œë ¤ìš”.</response>\n",
    "        <next> í˜¹ì‹œ ì–´ë¥´ì‹ ê»˜ì„œ ì•½ì„ ë“œì…¨ëŠ”ì§€ ë§ì”€í•´ì£¼ì‹¤ ìˆ˜ ìˆì„ê¹Œìš”?</next>\n",
    "\n",
    "        \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    final_message = messages + [user_input]\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "            final_message,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=4096\n",
    "        )\n",
    "\n",
    "    if isinstance(model_inputs, torch.Tensor):\n",
    "        input_ids = model_inputs\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    else:\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs[\"attention_mask\"]\n",
    "\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "    generation_outputs = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        attention_mask=attention_mask.to(\"cuda\"),\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(generation_outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    result = parse_llm_output(output_text)\n",
    "\n",
    "    if result:\n",
    "        print(result)\n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"parsed\": result\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"error\": \"ëª¨ë¸ ì¶œë ¥ íŒŒì‹± ì‹¤íŒ¨\",\n",
    "            \"raw_output\": output_text\n",
    "        }\n",
    "\n",
    "# ngrok ì—°ê²° ë° ì„œë²„ ì‹¤í–‰\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"ğŸš€ ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•˜ë ¤ë©´: {public_url}/api/inference\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1470e97926fc49198bad4ef62f937a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e55dfbfdba04f6aacaf47f5c7d3ed05",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9f4a9b332804723bde672a171a258c0",
      "value": 4
     }
    },
    "1e55dfbfdba04f6aacaf47f5c7d3ed05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2710ccc7aca1462f94ec328a4f34eae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27543a9624724590963d3afe8eadd5aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5487445c8f854b8680e43e287adad01e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93b2e1e0ca5f4f208770d9b8870d6237",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2710ccc7aca1462f94ec328a4f34eae4",
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "93b2e1e0ca5f4f208770d9b8870d6237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a799d3523f654e419ac97a114e5762d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab3cf5b3a7054ff492416b84737695d8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e9623dd3341c4ee791674ee0094576ca",
      "value": "â€‡4/4â€‡[00:22&lt;00:00,â€‡â€‡5.66s/it]"
     }
    },
    "ab3cf5b3a7054ff492416b84737695d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d78514ae88424e02a4c316ffbff20a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5487445c8f854b8680e43e287adad01e",
       "IPY_MODEL_1470e97926fc49198bad4ef62f937a22",
       "IPY_MODEL_a799d3523f654e419ac97a114e5762d9"
      ],
      "layout": "IPY_MODEL_27543a9624724590963d3afe8eadd5aa"
     }
    },
    "e9623dd3341c4ee791674ee0094576ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9f4a9b332804723bde672a171a258c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
