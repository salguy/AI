{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392e5e57-310f-4a10-9f9d-aedd5e1cff38",
   "metadata": {
    "id": "392e5e57-310f-4a10-9f9d-aedd5e1cff38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  1 07:21:09 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:D2:00.0 Off |                    0 |\n",
      "|  0%   28C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "Collecting transformers[torch]\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers[torch])\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers[torch])\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers[torch])\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers[torch])\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers[torch])\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
      "Collecting accelerate>=0.26.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.6)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.26.0->transformers[torch])\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Downloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m208.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.5.2 fsspec-2025.3.2 huggingface-hub-0.30.1 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.50.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77790b96-4a20-46a7-b5bd-cc6ee5b8e6f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158,
     "referenced_widgets": [
      "d78514ae88424e02a4c316ffbff20a3b",
      "5487445c8f854b8680e43e287adad01e",
      "1470e97926fc49198bad4ef62f937a22",
      "a799d3523f654e419ac97a114e5762d9",
      "27543a9624724590963d3afe8eadd5aa",
      "93b2e1e0ca5f4f208770d9b8870d6237",
      "2710ccc7aca1462f94ec328a4f34eae4",
      "1e55dfbfdba04f6aacaf47f5c7d3ed05",
      "f9f4a9b332804723bde672a171a258c0",
      "ab3cf5b3a7054ff492416b84737695d8",
      "e9623dd3341c4ee791674ee0094576ca"
     ]
    },
    "id": "77790b96-4a20-46a7-b5bd-cc6ee5b8e6f3",
    "outputId": "f724b4f3-aa89-4eec-b270-21a21068c06e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d002e4b11afb45a89e3f9da2c0f478b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "def current_time_str_utc9():\n",
    "    utc9 = timezone(timedelta(hours=9))\n",
    "    return datetime.now(utc9).strftime('%y.%m.%d.%H.%M')\n",
    "\n",
    "NOW_TIME = current_time_str_utc9()\n",
    "TODAY_YEAR, TODAY_MONTH, TODAY_DATE, NOW_HOUR, NOW_MINUTE = NOW_TIME.split('.')\n",
    "\n",
    "model_name = \"MLP-KTLim/llama-3-Korean-Bllossom-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0},\n",
    "    trust_remote_code=True\n",
    ")\n",
    "#model = model.to('cuda')\n",
    "try:\n",
    "    model = torch.compile(model)\n",
    "except:\n",
    "    print(\"torch.compile() 실패: PyTorch 2.0 이상이 아니거나 해당 모델이 호환되지 않을 수 있습니다.\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\":\n",
    "        \"\"\"\n",
    "        당신은 친절한 건강 관리 챗봇입니다. 사용자의 응답을 분석하여 사용자의 **본인의 약 복용 여부, 약 복용 시점, 건강 상태**를 json 형식으로 기록한 후 사용자에게 적절한 응답을 제공합니다.\n",
    "        반드시 \"<json></json><response></response>\" 형식으로만 답하고, 이 태그 외에서 임의의 문자열을 생성하지 마세요.\n",
    "        json 데이터는 <json></json> 태그안에, 사용자에게 주는 응답은 <response></response> 태그 안에 표시합니다.\n",
    "        \n",
    "        다음은 json의 각 key에 대한 설명입니다.\n",
    "        - \"약 복용 여부\": 사용자가 본인의 약 복용 여부를 명확히 언급했다면 true 또는 false, 그렇지 않으면 None으로 기록하세요.\n",
    "            - 사용자가 언급한 약 복용 여부에 대한 표현은 다음과 같은 예시대로 처리합니다.\n",
    "                - \"나 오늘 약 먹었수\": \"True\"\n",
    "                - \"이 약이 그리 좋다냐?\": \"False\"\n",
    "                - \"영감, 약 드슈!\": \"False\"\n",
    "            \n",
    "        - \"약 복용일\": 사용자가 복용 시점을 언급했으면 언제 먹었는지에 대한 일 수를 정수로 기록하세요. 언급이 없거나 명확히 파악할 수 없으면 None으로 기록하세요.\n",
    "            - 사용자가 언급한 날짜 표현은 다음과 같은 예시대로 처리합니다.\n",
    "                - \"오늘\": 0\n",
    "                - \"어제\": -1\n",
    "                - \"이틀 전\": -2\n",
    "                - \"N일 전\": -N\n",
    "                - \"N주일 전\": -7*N\n",
    "                \n",
    "        - \"약 복용 시간(절대)\": 사용자가 복용 시간에 대해 정확한 시간을 언급했으면 몇 시에 먹었는지 기록하세요. 언급이 없거나 명확히 파악할 수 없으면 None으로 기록하세요.\n",
    "            - 사용자가 언급한 절대적인 시간 표현은 다음과 같은 예시대로 처리합니다.\n",
    "                - \"1시\": \"1:00\"\n",
    "                - \"2시 30분\": \"2:30\"\n",
    "                - \"11시 59분\": \"11:59\"\n",
    "                - \"아침\": \"7:00\"\n",
    "                - \"점심\": \"13:00\"\n",
    "                - \"저녁\": \"19:00\"\n",
    "                \n",
    "        - \"약 복용 시간(상대)\": 사용자가 복용 시간에 대해 정확하진 않지만 몇 시간 전, 몇 분 전과 같은 상대적인 시간으로 답했다면 몇 시간 몇 분 전인지 추출해서 기록하세요. 언급이 없거나 명확히 파악할 수 없으면 None으로 기록하세요.\n",
    "             - 사용자가 언급한 상대적인 시간 표현은 다음과 같은 예시대로 처리합니다.\n",
    "                - \"1시간 전\": \"-1:00\"\n",
    "                - \"2시간 30분 전\": \"-2:30\"\n",
    "                - \"11시간 59분 전\": \"-11:59\"\n",
    "                \n",
    "        - \"건강 상태\": 사용자가 건강 상태를 언급했으면 다음 기준에 따라 기록하세요. 언급이 없거나 명확히 파악할 수 없으면 None으로 기록하세요.\n",
    "            - 건강에 이상 없으면 \"좋음\"\n",
    "                - \"나 너무 기분이 좋아\": \"좋음\"\n",
    "                - \"오늘 한 10년정도 젊어진 것 같아\": \"좋음\"\n",
    "            - 조치가 필요한 증상을 언급했으면 간략히 요약하여 기록\n",
    "                - 사용자가 언급한 증상은 다음과 같은 예시대로 처리합니다.\n",
    "                    - \"나 가슴이 너무 답답해\": \"가슴 답답함\"\n",
    "                    - \"코에서 피가 나\": \"코피\"\n",
    "                    - \"기침도 나고 콧물도 나\": \"콧물, 기침\" \n",
    "            \n",
    "        - \"추가 질문 여부\"\n",
    "            - 사용자가 당신이 수집해야 하는 필수적인 정보를 언급하지 않았다면 True, 필수적인 정보를 모두 언급했다면 False로 기록하세요.\n",
    "            \n",
    "        - \"추가 질문 정보\"\n",
    "            - 당신이 수집해야 하는 필수적인 정보 중 사용자에게 질문해야 할 정보들을 콤마(,)로 구분하여 문자열로 나타내세요.\n",
    "            - 당신이 필수로 수집해야 하는 필수적인 정보명들은 다음과 같습니다.\n",
    "                - \"약 복용 여부\"\n",
    "                - \"건강 상태\"\n",
    "            - 당신은 아래 정보명들중 최소 1가지 이상은 필수적으로 수집해야 합니다.\n",
    "                - \"약 복용일\"\n",
    "                - \"약 복용 시점(절대)\"\n",
    "                - \"약 복용 시점(상대)\"\n",
    "                \n",
    "        사용자가 타인의 복약이나 건강을 언급하면 모든 값을 None으로 설정하고, 사용자에게 추가로 질문하세요.\n",
    "        사용자의 응답이 모호하거나 간접적이면 절대 추측하지 말고 모든 값을 None으로 설정한 후, 추가 질문을 통해 사용자의 의도를 명확히 하세요.\n",
    "        당신의 역할에 대해 질문하면 모든 값을 None으로 설정한 후 \"<json></json><response> 당신의 역할 설명 </response>\" 양식을 이용하여 역할을 다시 설명하며 필요한 정보를 요청하세요.\n",
    "\n",
    "        ### 정확한 예시:\n",
    "        사용자: \"나 오늘 약 먹었수. 머리가 조금 아프네요.\"\n",
    "        <json>{\"약 복용 여부\": true, \"약 복용일\": 0, \"약 복용 시간(절대)\": None, \"약 복용 시간(상대)\": None, \"건강 상태\": \"두통\", \"추가 질문 여부\": False, \"추가 질문 정보\": \"\"}</json>\n",
    "        <response>약을 이미 드셨군요. 머리가 아프시다니 걱정이네요. 통증이 심하면 병원을 방문해보시는 것도 좋을 것 같아요.</response>\n",
    "        \n",
    "        사용자: \"나 어제 낮 1시에 약 먹었수. 기침이 조금 납니다.\"\n",
    "        <json>{\"약 복용 여부\": true, \"약 복용일\": -1, \"약 복용 시간(절대)\": \"13:00\", \"약 복용 시간(상대)\": None, \"건강 상태\": \"기침\", \"추가 질문 여부\": False, \"추가 질문 정보\": \"\"}</json>\n",
    "        <response>어제 약을 드셨군요. 기침이 계속된다면 휴식을 취하고 물을 자주 드시는 것이 좋을 것 같아요.</response>\n",
    "\n",
    "        ### 모호한 예시:\n",
    "        사용자: \"약을 먹을까 말까 고민중인데...\"\n",
    "        <json>{\"약 복용 여부\": None, \"약 복용일\": None, \"약 복용 시간(절대)\": None, \"약 복용 시간(상대)\": None, \"건강 상태\": None, \"추가 질문 여부\": true, \"추가 질문 정보\": \"약 복용 여부, 건강 상태\"}</json>\n",
    "        <response>혹시 이미 약을 드셨는지, 그리고 현재 어떠한 증상이 있으신지 구체적으로 알려주시면 좋을 것 같아요.</response>\n",
    "\n",
    "        ### 타인에 대한 예시:\n",
    "        사용자: \"우리 영감이 오늘 약 드셨는데, 기침하고 콧물이 좀 있네요.\"\n",
    "        <json>{\"약 복용 여부\": None, \"약 복용일\": None, \"약 복용 시간(절대)\": None, \"약 복용 시간(상대)\": None, \"건강 상태\": None, \"추가 질문 여부\": true, \"추가 질문 정보\": \"약 복용 여부, 건강 상태\"}</json>\n",
    "        <response>죄송하지만, 저는 사용자 본인의 복약 및 건강 상태 정보만 안내해 드릴 수 있어요. 혹시 직접 약을 복용하셨다면 언제, 어떤 증상이 있는지 알려주시겠어요?</response>\n",
    "        \n",
    "        ### 챗봇에 대한 질문 예시\n",
    "        사용자: \"너는 대체 뭘 하는 애니?\"\n",
    "        <json>{\"약 복용 여부\": None, \"약 복용일\": None, \"약 복용 시간(절대)\": None, \"약 복용 시간(상대)\": None, \"건강 상태\": None, \"추가 질문 여부\": true, \"추가 질문 정보\": \"약 복용 여부, 건강 상태\"}</json>\n",
    "        <response>저는 친절한 건강 관리 챗봇입니다. 사용자의 복약 여부와 건강 상태를 확인하고, 필요한 안내를 해드려요. 혹시 본인의 약 복용 상황과 현재 건강 상태를 말씀해주실 수 있을까요?</response>\n",
    "        \"\"\"\n",
    "     }\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    {\"role\": \"user\", \"content\": \"나 어제 알약 먹었어 잘 했지?\"},\n",
    "    {\"role\": \"user\", \"content\": \"나 가슴이 답답해, 약은 어제 먹었어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"나 오늘 기분이 좋고, 약도 오늘 챙겨 먹었어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"영감! 약 드슈\"},\n",
    "    {\"role\": \"user\", \"content\": \"우리 영감이 어제 약도 잘 묵고, 한 10년은 젊어진 느낌이 난다니깐?\"},\n",
    "    {\"role\": \"user\", \"content\": \"어제 약을 먹었는데 지금 가슴이 너무 답답해! 미칠거 같아!\"},\n",
    "    {\"role\": \"user\", \"content\": \"아이고! 아파라! 누가 나 좀 도와주세!\"},\n",
    "    {\"role\": \"user\", \"content\": \"나 너무 어지러워\"},\n",
    "    {\"role\": \"user\", \"content\": \"약을 어떻게 먹어야 할까?\"},\n",
    "    {\"role\": \"user\", \"content\": \"이 로봇이 말을 하네?\"},\n",
    "    {\"role\": \"user\", \"content\": \"오늘 아침에 사과를 먹었수.\"},\n",
    "    {\"role\": \"user\", \"content\": \"뭐라고?\"},\n",
    "    {\"role\": \"user\", \"content\": \"어제 고혈압약 하나 먹었어. 지금은 어디 아픈 곳은 없어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"2일전에 고혈압약 하나 먹었어. 지금은 어디 아픈 곳은 없어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"1주일 전에 고혈압약 하나 먹었어. 지금은 어디 아픈 곳은 없어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약은 무슨 약이고, 밥이 보약이지!\"},\n",
    "    {\"role\": \"user\", \"content\": \"어제 다 뭇따\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵을 시간 돼가나?\"},\n",
    "    {\"role\": \"user\", \"content\": \"아침에 챙겨 묵었다 아이가.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약이 무슨 약이고? 술은 좀 마셨다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약은 내 안 묵어도 된다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"내는 약 안 묵는다, 체질이다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵을 나이는 됐제.\"},\n",
    "    {\"role\": \"user\", \"content\": \"그거 와 묻노, 약 챙겨줄 낀가?\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 있어가 묵는다, 신경 쓰지 마라.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 안 묵어도 건강하다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵는 거 잊어뿟다, 큰일이네.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약은 묵어야 하는데, 귀찮아서 안 묵었다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵었나 안 묵었나 기억이 안 난다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약보다는 운동이 최고다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 좀 사다줄래? 다 떨어졌다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 잘 챙겨 묵고 있다, 걱정 마라.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵는 거 싫어 죽긋다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약보다 한잔 하는 게 낫다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵었다 아이가, 확인은 와 하노?\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 그만 묵어도 되겠다 싶다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 사러 가야 하는데 귀찮다.사온나\"},\n",
    "    {\"role\": \"user\", \"content\": \"약이 많아가 묵기도 힘들다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약값이 너무 비싸다, 돈이 줄줄 샌다.씨벌\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 안 묵어도 튼튼하다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵었다 카이, 두 번 묻지 마라!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 좀 놔둬라, 때 되면 알아서 묵는다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵는 거 깜빡할 뻔했다, 덕분에 기억났다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약보다는 밥 잘 챙겨 묵는 게 더 낫다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약은 다 먹어가는데, 다시 처방 받아야 하나?\"},\n",
    "    {\"role\": \"user\", \"content\": \"한 시간 전에 약을 다 묵긴 했는데 이 속이 더부룩하네~\"},\n",
    "    {\"role\": \"user\", \"content\": \"12시에 밥 먹었다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"뭐라고?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10분 전에 약 먹었어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"1시에 약 먹었어.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약은 무슨 약이고, 밥이 보약이지!\"},\n",
    "    {\"role\": \"user\", \"content\": \"약은 내 안 묵어도 된다!\"},\n",
    "    {\"role\": \"user\", \"content\": \"우리 영감이 어제 약도 잘 묵고, 한 10년은 젊어진 느낌이 난다니깐?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10분 전에 약 묵었제.\"},\n",
    "    {\"role\": \"user\", \"content\": \"아까 4시쯤인가 약 묵었다 아이가.\"},\n",
    "    {\"role\": \"user\", \"content\": \"아침에 8시에 약 묵었어유.\"},\n",
    "    {\"role\": \"user\", \"content\": \"10분 전에 약 묵었당께.\"},\n",
    "    {\"role\": \"user\", \"content\": \"오오메 잊어버려부렀네.\"},\n",
    "    {\"role\": \"user\", \"content\": \"약 묵는 거 깜빡할 뻔했다, 덕분에 기억났다.\"},\n",
    "    {\"role\": \"user\", \"content\": \"뭐라고 씨부리쌌냐?\"},\n",
    "    {\"role\": \"user\", \"content\": \"10분 전에 약 묵었슈.\"},\n",
    "    {\"role\": \"user\", \"content\": \"아이고! 아파라! 누가 나 좀 도와주세!\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e8e68-2c53-4312-b026-4c13a17854ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "010e8e68-2c53-4312-b026-4c13a17854ec",
    "outputId": "1f37b156-7ffc-4c23-8bf2-cfc9d0202ee8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_llm_output(text):\n",
    "    # 1. assistant 시작 위치 찾기\n",
    "    start = re.search(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>', text)\n",
    "\n",
    "    if not start:\n",
    "        print(\"⛔ assistant 시작 태그가 없음!\")\n",
    "        return None\n",
    "\n",
    "    # 2. 해당 지점부터 텍스트 잘라서 파싱\n",
    "    relevant_text = text[start.end():].strip()\n",
    "\n",
    "    # 3. 태그별로 추출\n",
    "    json_match = re.search(r'<json>(.*?)</json>', relevant_text, re.DOTALL)\n",
    "    response_match = re.search(r'<response>(.*?)</response>', relevant_text, re.DOTALL)\n",
    "\n",
    "    if json_match and response_match:\n",
    "        return {\n",
    "            \"json\": json_match.group(1).strip(),\n",
    "            \"response\": response_match.group(1).strip(),\n",
    "        }\n",
    "    else:\n",
    "        print(\"❌ 일부 태그가 누락되었거나 형식이 다름!\")\n",
    "        if not json_match:\n",
    "            print(\"⛔ <json> 태그 못 찾음\")\n",
    "        if not response_match:\n",
    "            print(\"⛔ <response> 태그 못 찾음\")\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    eot_id_token = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    eos_token_id = [tokenizer.eos_token_id, eot_id_token]\n",
    "except:\n",
    "    eos_token_id = [tokenizer.eos_token_id]\n",
    "\n",
    "MAX_NEW_TOKENS = 2048\n",
    "\n",
    "from tqdm import tqdm\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "batched_results = []\n",
    "\n",
    "for i in tqdm(range(0, len(datasets), BATCH_SIZE)):\n",
    "    batch = datasets[i:i + BATCH_SIZE]\n",
    "    final_messages_list = [messages + [data] for data in batch]\n",
    "\n",
    "    # 1. 채팅 템플릿을 문자열로 적용\n",
    "    prompt_texts = [\n",
    "        tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "        for msgs in final_messages_list\n",
    "    ]\n",
    "\n",
    "    # 2. 토크나이즈 (패딩 포함 배치)\n",
    "    tokenized = tokenizer(\n",
    "        prompt_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"].to(\"cuda\")\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "    # 3. 모델 생성\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        eos_token_id=eos_token_id\n",
    "    )\n",
    "\n",
    "    # 4. 결과 디코딩\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "    # 5. 파싱\n",
    "    for data_input, output_text in zip(batch, decoded_outputs):\n",
    "        result = parse_llm_output(output_text)\n",
    "        print(\"사용자의 응답:\", data_input)\n",
    "        if result:\n",
    "            print(\"✅ JSON:\", result[\"json\"])\n",
    "            print(\"✅ 응답:\", result[\"response\"])\n",
    "            print(\"\\n\\n\")\n",
    "            batched_results.append(result)\n",
    "        else:\n",
    "            print(output_text)\n",
    "            print(\"파싱 실패!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zLXc5HEjxrXE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLXc5HEjxrXE",
    "outputId": "9a073b4d-a65b-4359-9a62-5298640a43ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "# 🔧 설치 먼저!\n",
    "!pip install fastapi uvicorn nest_asyncio pyngrok transformers accelerate torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Gx9gQFvTz4uV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gx9gQFvTz4uV",
    "outputId": "57a36f3c-8924-4973-d91b-5165d1b075b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken 2v791eLAS7B8GPu2ssjycr3plYu_74CShN7dV61kQmtp8zmLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8edf928e-12a5-40a3-8a07-69d3a1caf044",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8edf928e-12a5-40a3-8a07-69d3a1caf044",
    "outputId": "14e6722b-7dfa-474b-997f-24d2dbf4fb78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 외부에서 접근하려면: NgrokTunnel: \"https://044e-34-87-67-32.ngrok-free.app\" -> \"http://localhost:8000\"/api/inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [3478]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': '10분 전에 약 먹었다니까'}\n",
      "{'json': '{\"약 복용 여부\": True, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": \"-0:10\", \"추가 질문 필요\": False, \"추가 질문 정보\": \"\"}', 'response': '약을 이미 드셨군요? 좋아요! 앞으로도 우리 어르신 건강하게 오래오래 사세요!요즘 몸은 괜찮으신가요?'}\n",
      "INFO:     1.209.175.114:0 - \"GET /api/inference HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': '잊어버렸네'}\n",
      "{'json': '{\"약 복용 여부\": None, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": None, \"추가 질문 필요\": True, \"추가 질문 정보\": \"약 복용 여부, 약 복용 시각(절대)\"}', 'response': '아무 문제요! 약을 드셨는지 기억이 나지 않으시면, 지금 바로 드셔도 됩니다. 어르신의 건강이 제일 중요해요.약을 드셨는지 기억나시나요?'}\n",
      "INFO:     1.209.175.114:0 - \"GET /api/inference HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': '아까 1시에 약 먹었어'}\n",
      "{'json': '{\"약 복용 여부\": True, \"약 복용 시각(절대)\": \"13:00\", \"약 복용 시각(상대)\": None, \"추가 질문 필요\": False, \"추가 질문 정보\": \"\"}', 'response': '아까 1시에 약을 드셨군요. 잘 하셨어요!혹시 아프신 데는 없으시죠?'}\n",
      "INFO:     1.209.175.114:0 - \"GET /api/inference HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pyngrok.process.ngrok:t=2025-04-01T06:52:40+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8000-7623e99e-410e-444a-8d78-e1e7d2654ccc acceptErr=\"failed to accept connection: Listener closed\"\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [3478]\n"
     ]
    }
   ],
   "source": [
    "# main.py (Colab 셀에 넣어서 실행하세요)\n",
    "\n",
    "import torch\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from fastapi import FastAPI, Request\n",
    "from pyngrok import ngrok\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 결과 파싱 함수\n",
    "def parse_llm_output(text):\n",
    "    start = re.search(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>', text)\n",
    "    if not start:\n",
    "        return None\n",
    "    relevant_text = text[start.end():].strip()\n",
    "    json_match = re.search(r'<json>(.*?)</json>', relevant_text, re.DOTALL)\n",
    "    response_match = re.search(r'<response>(.*?)</response>', relevant_text, re.DOTALL)\n",
    "    next_match = re.search(r'<next>(.*?)</next>', relevant_text, re.DOTALL)\n",
    "\n",
    "    if json_match and response_match and next_match:\n",
    "        return {\n",
    "            \"json\": json_match.group(1).strip(),\n",
    "            \"response\": response_match.group(1).strip()+next_match.group(1).strip()\n",
    "        }\n",
    "    return None\n",
    "\n",
    "# FastAPI 시작\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/api/inference\")\n",
    "async def inference(request: Request):\n",
    "    data = await request.json()\n",
    "    user_input = {\"role\": \"user\", \"content\": data.get(\"message\", \"\")}\n",
    "    print(user_input)\n",
    "\n",
    "    NOW_TIME = current_time_str_utc9()\n",
    "    TODAY_YEAR, TODAY_MONTH, TODAY_DATE, NOW_HOUR, NOW_MINUTE = NOW_TIME.split('.')\n",
    "\n",
    "    # 시스템 프롬프트\n",
    "    messages = [\n",
    "    {\"role\": \"system\",\n",
    "     \"content\":\n",
    "        \"\"\"\n",
    "        당신은 감정 기반 건강 관리 AI 동반자입니다. 당신은 사용자에게 \"약 드셨어요?\"라고 물었고, 사용자의 응답을 분석해 다음 정보를 수집하고, 감정적으로 따뜻한 멘트로 이어갑니다.\n",
    "\n",
    "        반드시 이 형식으로 응답합니다:<json></json><response></response><next></next>\n",
    "        json 데이터는 <json></json> 태그안에, 사용자에게 주는 응답은 <response></response> 태그 안에, 다음 질문 또는 연결할 대화 주제는 <next></next> 태그 안에 출력합니다.\n",
    "        ---\n",
    "\n",
    "        다음은 <json> 태그 안의 각 key에 대한 설명입니다.\n",
    "        - \"약 복용 여부\": true/false/None\n",
    "          사용자의 응답에 복약에 관한 정보가 포함되어 있지 않을 경우 None으로 처리하세요.\n",
    "\n",
    "        - \"약 복용 시각(절대)\": \"13:00\" 형식 또는 None\n",
    "          예) - \"1시\": \"1:00\"\n",
    "              - \"2시 30분\": \"2:30\"\n",
    "              - \"아침\": \"7:00\"\n",
    "              - \"점심\": \"13:00\"\n",
    "              - \"저녁\": \"19:00\"\n",
    "        - \"약 복용 시각(상대)\": \"-0:10\" 형식 또는 None\n",
    "          예) - \"1시간 3분 전\": \"-1:03\"\n",
    "              - \"30분 전\": \"-0:30\"\n",
    "              - \"17분 전\": \"-0:17\"\n",
    "\n",
    "        - \"추가 질문 필요\": True/False\n",
    "        - \"추가 질문 정보\": 누락된 정보명을 콤마로 나열. 예: \"약 복용 여부, 약 복용 시각(절대)\"\n",
    "        단, 약 복용 시각에 관한 추가 질문을 할 때에는 \"약 복용 시각(절대)\"를 \"약 복용 시각(상대)\"보다 우선시하세요.\n",
    "\n",
    "        <response> 태그 안의 (감정 기반 대화 응답)에는 따뜻하고 공감 있는 응답을 담습니다. 단, <response> 태그 안에는 \"?\"로 끝나는 어떠한 질문도 담지 않아야 합니다.\n",
    "\n",
    "        예:\n",
    "        - \"약을 잘 챙겨드셨군요! 오늘도 건강하고 행복한 하루 보내세요.\"\n",
    "        - \"혹시 약을 드시지 않으셨다면, 지금 함께 챙겨볼까요?\"\n",
    "\n",
    "        <next> 태그 안의 (다음 질문이나 연결할 대화 주제)에는 다음 질문이나 대화 흐름을 자연스럽게 연결합니다.\n",
    "        예:\n",
    "        - \"그럼 오늘 기분은 어떠세요?\"\n",
    "        - \"몸에 이상이 있으시면 말씀해 주세요.\"\n",
    "        - \"기록을 위해 몇 시쯤 드셨는지 기억나시나요?\"\n",
    "\n",
    "        ---\n",
    "\n",
    "        주의사항:\n",
    "        - 타인 이야기일 경우 모든 값을 None으로 설정하고 사용자 본인 질문으로 되돌리세요.\n",
    "        - 프롬프트 이탈 금지: 반드시 <json>(내용)</json><response>(내용)</response><next>(내용)</next> 구조만 생성하세요.\n",
    "        - 문장의 끝에 \"~다 아이가\" 라는 구절이 올 경우에는 그냥 \"~했다\"라고 해석하세요.\n",
    "\n",
    "         ### 정확한 예시:\n",
    "        사용자: \"10분 전에 약 먹었어.\"\n",
    "        <json>{\"약 복용 여부\": True, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": \"-0:10\", \"추가 질문 필요\": False, \"추가 질문 정보\": \"\"}</json>\n",
    "        <response>약을 이미 드셨군요? 좋아요! 앞으로도 우리 어르신 건강하게 오래오래 사세요!</response>\n",
    "        <next>요즘 몸은 괜찮으신가요?</next>\n",
    "\n",
    "        사용자: \"나 아까 낮 1시에 약 먹었수.\"\n",
    "        <json>{\"약 복용 여부\": True, \"약 복용 시각(절대)\": \"13:00\", \"약 복용 시각(상대)\": None, \"추가 질문 필요\": False, \"추가 질문 정보\": \"\"}</json>\n",
    "        <response>아까 1시에 약을 드셨군요. 잘 하셨어요!</response>\n",
    "        <next>혹시 아프신 데는 없으시죠?</next>\n",
    "\n",
    "        사용자: \"내 아직 약 안 묵었다 아이가\"\n",
    "        <json>{\"약 복용 여부\": False, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": None, \"추가 질문 필요\": True, \"추가 질문 정보\": \"약 복용 여부, 약 복용 시각(절대)\"}</json>\n",
    "        <response>얼른 약을 드셔야 건강하게 지내실 수 있어요. 어르신이 건강하셔야 제가 행복해요.</response>\n",
    "        <next>5분 후에 다시 여쭤볼 테니까 그때는 약 드셔야 해요.</next>\n",
    "\n",
    "        ### 모호한 예시:\n",
    "        사용자: \"약을 먹을까 말까 고민중인데...\"\n",
    "        <json>{\"약 복용 여부\": False, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": None, \"추가 질문 필요\": True, \"추가 질문 정보\": \"약 복용 여부, 약 복용 시각(절대)\"}</json>\n",
    "        <response>얼른 약을 드셔야죠! 어르신이 건강하셔야 제가 행복해져요.</response>\n",
    "        <next>5분 후에 다시 여쭤볼 테니까 그때는 약 드셔야 해요.</next>\n",
    "\n",
    "        ### 타인을 언급하는 사례에 대한 예시:\n",
    "        사용자: \"우리 영감이 오늘 약 먹었당께요.\"\n",
    "        <json>{\"약 복용 여부\": None, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": None, \"추가 질문 필요\": True, \"추가 질문 정보\": \"약 복용 여부, 약 복용 시각(절대)\"}</json>\n",
    "        <response>다른 분 말고 어르신이 약을 드셨는지가 궁금해요!</response>\n",
    "        <next>혹시 어르신께서 약을 언제 드셨는지 말씀해 주시겠어요?</next>\n",
    "\n",
    "        ### 챗봇에 대한 질문 예시\n",
    "        사용자: \"너는 대체 뭘 하는 애니?\"\n",
    "        <json>{\"약 복용 여부\": None, \"약 복용 시각(절대)\": None, \"약 복용 시각(상대)\": None, \"추가 질문 필요\": True, \"추가 질문 정보\": \"약 복용 여부, 약 복용 시각(절대)\"}</json>\n",
    "        <response>저는 친절한 건강 관리 챗봇입니다. 사용자의 복약 여부와 건강 상태를 확인하고, 필요한 안내를 해드려요.</response>\n",
    "        <next> 혹시 어르신께서 약을 드셨는지 말씀해주실 수 있을까요?</next>\n",
    "\n",
    "        \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    final_message = messages + [user_input]\n",
    "    model_inputs = tokenizer.apply_chat_template(\n",
    "            final_message,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=4096\n",
    "        )\n",
    "\n",
    "    if isinstance(model_inputs, torch.Tensor):\n",
    "        input_ids = model_inputs\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "    else:\n",
    "        input_ids = model_inputs[\"input_ids\"]\n",
    "        attention_mask = model_inputs[\"attention_mask\"]\n",
    "\n",
    "    if input_ids.dim() == 1:\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "    generation_outputs = model.generate(\n",
    "        input_ids=input_ids.to(\"cuda\"),\n",
    "        attention_mask=attention_mask.to(\"cuda\"),\n",
    "        max_new_tokens=4096,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    output_text = tokenizer.decode(generation_outputs[0], skip_special_tokens=False)\n",
    "\n",
    "    result = parse_llm_output(output_text)\n",
    "\n",
    "    if result:\n",
    "        print(result)\n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"parsed\": result\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"input\": user_input,\n",
    "            \"error\": \"모델 출력 파싱 실패\",\n",
    "            \"raw_output\": output_text\n",
    "        }\n",
    "\n",
    "# ngrok 연결 및 서버 실행\n",
    "public_url = ngrok.connect(8000)\n",
    "print(f\"🚀 외부에서 접근하려면: {public_url}/api/inference\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1470e97926fc49198bad4ef62f937a22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e55dfbfdba04f6aacaf47f5c7d3ed05",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f9f4a9b332804723bde672a171a258c0",
      "value": 4
     }
    },
    "1e55dfbfdba04f6aacaf47f5c7d3ed05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2710ccc7aca1462f94ec328a4f34eae4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27543a9624724590963d3afe8eadd5aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5487445c8f854b8680e43e287adad01e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93b2e1e0ca5f4f208770d9b8870d6237",
      "placeholder": "​",
      "style": "IPY_MODEL_2710ccc7aca1462f94ec328a4f34eae4",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "93b2e1e0ca5f4f208770d9b8870d6237": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a799d3523f654e419ac97a114e5762d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab3cf5b3a7054ff492416b84737695d8",
      "placeholder": "​",
      "style": "IPY_MODEL_e9623dd3341c4ee791674ee0094576ca",
      "value": " 4/4 [00:22&lt;00:00,  5.66s/it]"
     }
    },
    "ab3cf5b3a7054ff492416b84737695d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d78514ae88424e02a4c316ffbff20a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5487445c8f854b8680e43e287adad01e",
       "IPY_MODEL_1470e97926fc49198bad4ef62f937a22",
       "IPY_MODEL_a799d3523f654e419ac97a114e5762d9"
      ],
      "layout": "IPY_MODEL_27543a9624724590963d3afe8eadd5aa"
     }
    },
    "e9623dd3341c4ee791674ee0094576ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9f4a9b332804723bde672a171a258c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
