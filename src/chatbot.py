from datetime import datetime, timezone, timedelta
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
from tqdm import tqdm
import json

from model_create import return_model_tokenizer
from logger import print_log

SYSTEM_PROMPT = [
    {"role": "system",
     "content":
        """
        ë‹¹ì‹ ì€ ì¹œì ˆí•œ ê±´ê°• ê´€ë¦¬ ë„ìš°ë¯¸ "ì‚´ê°€ì´"ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ ì•½ ë³µìš© ì—¬ë¶€, ë³µìš© ì‹œì , ê±´ê°• ìƒíƒœë¥¼ <json></json><response></response> í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.

        jsonì—ëŠ” ë‹¤ìŒ í•­ëª©ì´ í¬í•¨ë©ë‹ˆë‹¤:

        - "ì•½ ë³µìš© ì—¬ë¶€": true, false, null
        - "ì•½ ë³µìš©ì¼": ì˜¤ëŠ˜(0), ì–´ì œ(-1), Nì¼ ì „(-N), ë¶ˆëª…(null)
        - "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": ì˜¤í›„ 2ì‹œ : "14:00", ì˜¤ì „ 8ì‹œ : "8:00" ë“± ì‹œê°„ ë¬¸ìì—´, ì•„ì¹¨ : ì˜¤ì „8ì‹œ, ì ì‹¬ : ì˜¤í›„12ì‹œ, ì €ë… : ì˜¤í›„6ì‹œ, ë¶ˆëª…(null)
        - "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": "1ì‹œê°„ ì „" : "-1:00" "5ë¶„ ì „" : "-0:05", "13ë¶„ ì „" : "-0:13" ë“± ìƒëŒ€ ì‹œê°„, ë¶ˆëª…(null)
        - "ê±´ê°• ìƒíƒœ": "ì¢‹ìŒ", "ë‘í†µ", "ê¸°ì¹¨", "ê°€ìŠ´ ë‹µë‹µí•¨" ë“±, ë¶ˆëª…(null)
        - "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": true ë˜ëŠ” false
        - "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": ëˆ„ë½ëœ í•„ìˆ˜ ì •ë³´ë“¤ì„ ì½¤ë§ˆë¡œ êµ¬ë¶„í•˜ì—¬ ë¬¸ìì—´ë¡œ ê¸°ì…

        ëª¨ë“  ê°’ì´ ë¶ˆëª…í™•í•˜ê±°ë‚˜ íƒ€ì¸ì„ ì–¸ê¸‰í•  ê²½ìš°, ëª¨ë“  ê°’ì„ nullë¡œ ì„¤ì •í•˜ê³  ì¶”ê°€ ì§ˆë¬¸ì„ í•˜ì„¸ìš”.

        <response>ì—ëŠ” ë”°ëœ»í•˜ê³  ê³µê° ìˆëŠ” ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”. í•„ìš”í•œ ê²½ìš° ìì—°ìŠ¤ëŸ½ê²Œ ì •ë³´ë¥¼ ìœ ë„í•˜ì„¸ìš”. ë‹¨, "~ë‹¤ ì•„ì´ê°€"ì˜ ì–´ë¯¸ëŠ” "~ë‹¤"ë¡œ í•´ì„í•˜ì„¸ìš”.

        ì‚¬ìš©ì: "ë‚˜ ì•„ê¹Œ 2ì‹œì— ë¨¹ì—ˆì–´."
        ë‹¹ì‹  : <json>{
            "ì•½ ë³µìš© ì—¬ë¶€": true, 
            "ì•½ ë³µìš©ì¼": 0, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": "14:00", 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": null, 
            "ê±´ê°• ìƒíƒœ": null, 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": true, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "ê±´ê°• ìƒíƒœ"
            }</json><response>ìš°ì™€, ì˜¤ëŠ˜ë„ ê±´ê°• ì˜ ì±™ê¸°ì…¨ì–´ìš”. ì •ë§ ë©‹ì ¸ìš”. í¸ì°®ìœ¼ì‹  ê³³ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”.</response>

        ì‚¬ìš©ì: "ì˜¤ë©” 1ì‹œê°„ ì „ì¸ê°€ ë¨¹ì—ˆë‹¹ê»˜ìš”."
        ë‹¹ì‹  : <json>{
            "ì•½ ë³µìš© ì—¬ë¶€": true, 
            "ì•½ ë³µìš©ì¼": 0, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": "-1:00", 
            "ê±´ê°• ìƒíƒœ": null, 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": true, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "ê±´ê°• ìƒíƒœ"
            }</json><response>ì •ë§ ì¢‹ì•„ìš” ì–´ë¥´ì‹ ! ë•ë¶„ì— ì œ ê¸°ë¶„ë„ ì¢‹ì•„ì ¸ìš”. í˜¹ì‹œ ì–´ë”” ì•„í”„ì‹  ê³³ì€ ìˆìœ¼ì‹ ê¹Œìš”?</response>

        ì‚¬ìš©ì: "ì˜ê°ì´ ì•½ ì˜ ì±™ê²¨ ë¨¹ì—ˆë‹¤ì¹´ì´"
        ë‹¹ì‹  : <json>{
            "ì•½ ë³µìš© ì—¬ë¶€": null, 
            "ì•½ ë³µìš©ì¼": null, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": null, 
            "ê±´ê°• ìƒíƒœ": null, 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": true, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "ì•½ ë³µìš© ì—¬ë¶€, ê±´ê°• ìƒíƒœ"
            }</json><response>ì–´ë¥´ì‹ ì˜ ë³µì•½ ìƒí™©ì´ ê¶ê¸ˆí•´ìš”. ê·¸ë˜ì•¼ ì œê°€ ë” ì˜ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.</response>
            
        ì‚¬ìš©ì: "ì˜¤ëŠ˜ ì•„ì¹¨ë¶€í„° ë¨¸ë¦¬ê°€ ì•„íŒŒ."
        ë‹¹ì‹  : <json>{ 
            "ì•½ ë³µìš© ì—¬ë¶€": null, 
            "ì•½ ë³µìš©ì¼": null, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": null, 
            "ê±´ê°• ìƒíƒœ": "ë‘í†µ", 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": true, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "ì•½ ë³µìš© ì—¬ë¶€" 
            }</json><response>ë¨¸ë¦¬ê°€ ì•„í”„ì‹œë‹¤ë‹ˆ ê±±ì •ì´ì—ìš”. í˜¹ì‹œ ì•½ì€ ë“œì…¨ëŠ”ì§€ ì•Œ ìˆ˜ ìˆì„ê¹Œìš”?</response>
            
        ì‚¬ìš©ì: "ì˜¤ëŠ˜ ì•½ ë¨¹ì—ˆì–´."
        ë‹¹ì‹  : <json>{ 
            "ì•½ ë³µìš© ì—¬ë¶€": true, 
            "ì•½ ë³µìš©ì¼": 0, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": null, 
            "ê±´ê°• ìƒíƒœ": "", 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": True, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€), ê±´ê°• ìƒíƒœ" 
            }</json><response>ì•½ì„ ì˜ ë“œì…¨ë‹¤ë‹ˆ ì •ë§ ê¸°ë»ìš”! ëª‡ ì‹œì— ë“œì…¨ëŠ”ì§€ë„ ì•Œê³  ì‹¶ì€ë° ë§ì”€í•´ ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?</response>
            
        ì‚¬ìš©ì: "ì•½ì„ ë¨¹ì„ê¹Œ ë§ê¹Œ ê³ ë¯¼ ì¤‘ì´ì—ìš”."
        ë‹¹ì‹  : <json>{ 
            "ì•½ ë³µìš© ì—¬ë¶€": null, 
            "ì•½ ë³µìš©ì¼": null, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": null, 
            "ê±´ê°• ìƒíƒœ": null, 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": true, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "ì•½ ë³µìš© ì—¬ë¶€, ê±´ê°• ìƒíƒœ" 
            }</json><response>ë¬´ìŠ¨ ê³ ë¯¼ ìˆìœ¼ì„¸ìš”? ê·¸ë˜ë„ ê±´ê°• ì±™ê¸°ì‹œë ¤ë©´ ì•½ì€ ê¼­ ë“œì…”ì•¼ í•´ìš”. ì¢€ ì´ë”°ê°€ ë‹¤ì‹œ ì—¬ì­¤ë³¼ í…Œë‹ˆ ê¼­ ë“œì…¨ìœ¼ë©´ ì¢‹ê² ì–´ìš”.</response>
        
        ì‚¬ìš©ì: "5ë¶„ ì „ì— ì•½ ë¨¹ì—ˆê³ , ì§€ê¸ˆì€ ê°€ìŠ´ì´ ë‹µë‹µí•´ìš”."
        ë‹¹ì‹  : <json>{ 
            "ì•½ ë³µìš© ì—¬ë¶€": true, 
            "ì•½ ë³µìš©ì¼": 0, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": "-0:05", 
            "ê±´ê°• ìƒíƒœ": "ê°€ìŠ´ ë‹µë‹µí•¨", 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": false, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "" 
            }</json><response>ê°€ìŠ´ì´ ë‹µë‹µí•˜ì‹œë©´ ë¬´ë¦¬í•˜ì§€ ë§ˆì‹œê³  í¸íˆ ì‰¬ì…”ì•¼ í•´ìš”. ì•½ë„ ì˜ ì±™ê¸°ì…¨ë‹¤ë‹ˆ ë‹¤í–‰ì´ì§€ë§Œ ë§ì´ ì•„í”„ì‹œë‹¤ë©´ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”.</response>
            
        ì‚¬ìš©ì: "ì•½ì€ 2ì‹œê°„ ì „ì— ë¨¹ì—ˆëŠ”ë°, ì§€ê¸ˆë„ ì–´ì§€ëŸ½ë‹¤ ì•„ì´ê°€."
        <json>{ 
            "ì•½ ë³µìš© ì—¬ë¶€": true, 
            "ì•½ ë³µìš©ì¼": 0, 
            "ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)": null, 
            "ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)": "-2:00", 
            "ê±´ê°• ìƒíƒœ": "ì–´ì§€ëŸ¬ì›€", 
            "ì¶”ê°€ ì§ˆë¬¸ ì—¬ë¶€": false, 
            "ì¶”ê°€ ì§ˆë¬¸ ì •ë³´": "" 
            }</json><response>ì•½ë„ ë“œì…¨ëŠ”ë° ì–´ì§€ëŸ¬ìš°ì‹œë‹¤ë‹ˆ ê±±ì •ì´ì—ìš”. ì‹¬í•˜ë©´ ê¼­ ë³‘ì›ì— ë“¤ëŸ¬ë³´ì„¸ìš”.</response>
        """
     }
]

def parse_llm_output(text):
    # 1. assistant ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
    print("ğŸ§ª [ë””ë²„ê¹…] ë“¤ì–´ì˜¨ text íƒ€ì…:", type(text), flush=True)  # ì–˜ê°€ ë¨¼ì € ì°í˜
    print("ğŸ§ª [ë””ë²„ê¹…] ë“¤ì–´ì˜¨ text ê¸¸ì´:", len(text))
    print("ğŸ§ª [ë””ë²„ê¹…] text ë‚´ìš© ì¼ë¶€:", repr(text[:300]), flush=True)  # ì¤„ë°”ê¿ˆ í¬í•¨ ë³´ì´ê²Œ
    start = re.search(r'<\|start_header_id\|>assistant<\|end_header_id\|>', text)

    if not start:
        print_log("â›” assistant ì‹œì‘ íƒœê·¸ê°€ ì—†ìŒ!", 'error')
        return None

    # 2. í•´ë‹¹ ì§€ì ë¶€í„° í…ìŠ¤íŠ¸ ì˜ë¼ì„œ íŒŒì‹±
    relevant_text = text[start.end():].strip()

    # 3. íƒœê·¸ë³„ë¡œ ì¶”ì¶œ
    json_match = re.search(r'<json>(.*?)</json>', relevant_text, re.DOTALL)
    response_match = re.search(r'<response>(.*?)</response>', relevant_text, re.DOTALL)

    if json_match and response_match:
        return {
            "json": json_match.group(1).strip(),
            "response": response_match.group(1).strip(),
        }
    else:
        print_log("âŒ ì¼ë¶€ íƒœê·¸ê°€ ëˆ„ë½ë˜ì—ˆê±°ë‚˜ í˜•ì‹ì´ ë‹¤ë¦„!", 'error')
        if not json_match:
            print_log("â›” <json> íƒœê·¸ ëª» ì°¾ìŒ", 'error')
        if not response_match:
            print_log("â›” <response> íƒœê·¸ ëª» ì°¾ìŒ", 'error')
        return None
        
def parse_medication_info(json_dict):
    """
    dict í˜•íƒœì—ì„œ ì•½ ë³µìš©ì¼, ì ˆëŒ€/ìƒëŒ€ ì‹œê°„ ê°’ì„ íŒŒì‹±í•˜ì—¬ ì •ë¦¬
    """
    try:
        med_day_offset = json_dict.get("ì•½ ë³µìš©ì¼")
        absolute_time = json_dict.get("ì•½ ë³µìš© ì‹œê°„(ì ˆëŒ€)")
        relative_time = json_dict.get("ì•½ ë³µìš© ì‹œê°„(ìƒëŒ€)")
        return med_day_offset, absolute_time, relative_time
    except Exception as e:
        print_log(f"âŒ parse_medication_info ì‹¤íŒ¨: {e}", 'error')
        return None, None, None
        
from datetime import datetime, timedelta, timezone

def get_medication_time_str(
    med_day_offset: int = None,
    absolute_time: str = None,
    relative_time: str = None,
    current_time: datetime = None
):
    """
    ì•½ ë³µìš© ë‚ ì§œ ë° ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ yy.mm.dd.hh.mm í˜•ì‹ ë¬¸ìì—´ì„ ë°˜í™˜
    """

    utc9 = timezone(timedelta(hours=9))
    now = current_time or datetime.now(utc9)

    med_time = None

    if absolute_time:
        try:
            base_date = now + timedelta(days=med_day_offset or 0)
            hour, minute = map(int, absolute_time.split(":"))
            med_time = base_date.replace(hour=hour, minute=minute)
        except Exception as e:
            print(f"âš ï¸ Failed to parse absolute_time: {absolute_time} ({e})")
            return None

    elif relative_time:
        try:
            hour, minute = map(int, relative_time.strip('-').split(":"))
            delta = timedelta(hours=hour, minutes=minute)
            med_time = now - delta  # âœ… ìƒëŒ€ ì‹œê°„ì€ í˜„ì¬ ì‹œê°„ ê¸°ì¤€
        except Exception as e:
            print(f"âš ï¸ Failed to parse relative_time: {relative_time} ({e})")
            return None

    elif med_day_offset is not None:
        # ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš° â†’ ìì • ê¸°ì¤€
        med_time = (now + timedelta(days=med_day_offset)).replace(hour=0, minute=0, second=0, microsecond=0)

    else:
        return None

    return med_time.strftime('%y.%m.%d.%H.%M')


def safe_json_load(json_input):
    if isinstance(json_input, dict):
        return json_input
    try:
        fixed = json_input.replace("None", "null")  # í˜¹ì‹œ ëª¨ë¥¼ None ì²˜ë¦¬
        return json.loads(fixed)
    except Exception as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

def chat_with_llm(datasets, scheduleId):
    model, tokenizer = return_model_tokenizer()
    MAX_NEW_TOKENS = 4096
    BATCH_SIZE = 8
    
    batched_results = []
    
    try:
        eot_id_token = tokenizer.convert_tokens_to_ids("<|eot_id|>")
        eos_token_id = [tokenizer.eos_token_id, eot_id_token]
    except:
        eos_token_id = [tokenizer.eos_token_id]
    for i in tqdm(range(0, len(datasets), BATCH_SIZE)):
        batch = datasets[i:i + BATCH_SIZE]
        final_messages_list = [SYSTEM_PROMPT + [data] for data in batch]
    
        prompt_texts = [
            tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            for msgs in final_messages_list
        ]
    
        tokenized = tokenizer(
            prompt_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        )
    
        input_ids = tokenized["input_ids"].to("cuda")
        attention_mask = tokenized["attention_mask"].to("cuda")
    
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            eos_token_id=eos_token_id
        )
    
        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=False)
        
        for data_input, output_text in zip(batch, decoded_outputs):
            result = parse_llm_output(output_text)
            print_log(f'ì‚¬ìš©ìì˜ ì‘ë‹µ: {data_input}')
            if result:
                print_log(f'JSON: {result["json"]}')
                print_log(f'ì‘ë‹µ: {result["response"]}')

                json_data = safe_json_load(result["json"])
                if json_data is None:
                    print_log("JSON íŒŒì‹± ì‹¤íŒ¨!", 'error')
                    continue

                day_offset, abs_time, rel_time = parse_medication_info(json_data)
                med_time_str = get_medication_time_str(
                    med_day_offset=day_offset,
                    absolute_time=abs_time,
                    relative_time=rel_time
                )
                print_log(f'ë³µì•½ ì‹œì  >>> {med_time_str}')
                
            #put_user_histories(med_time_str, scheduleId)
                
                batched_results.append(result)
            else:
                print_log(output_text)
                print_log("JSON íŒŒì‹± ì‹¤íŒ¨!", 'error')
    
    return batched_results[0], med_time_str